# 均绝对误差(L1损失)

均绝对误差（Mean Absolute Error,MAE) 是指模型预测值f(x)和真实值y之间距离的均值，其公式如下：




$$
MAE=\frac{\sum^n_{n=1}∣f(x_i)−y_i∣}{n}
$$


忽略下标ii ，设$n=1$，以$y-f(x)$为横轴，$MAE$的值为纵轴，得到函数的图形如下：

![untitled](D:/oneDrive/typr/untitled-16628859960921.png)

# 均方误差MSE(L2损失)

均方误差（Mean Square Error,MSE）是模型预测值f(x)f(x) 与真实样本值yy 之间差值方的均值，其公式如下

其中，yiyi和f(xi)f(xi)分别表示第ii个样本的真实值及其对应的预测值，nn为样本的个数。

忽略下标ii ，设n=1n=1，以f(x)−yf(x)−y为横轴，MSE的值为纵轴，得到函数的图形如下：

![untitled](D:/oneDrive/typr/untitled.png)

MSE的函数曲线光滑、连续，处处可导，便于使用梯度下降算法，是一种常用的损失函数。 而且，随着误差的减小，梯度也在减小，这有利于收敛，即使使用固定的学习速率，也能较快的收敛到最小值。

当yy和f(x)f(x)也就是真实值和预测值的差值大于1时，会放大误差；而当差值小于1时，则会缩小误差，这是*方运算决定的。MSE对于较大的误差（>1>1）给予较大的惩罚，较小的误差（<1<1）给予较小的惩罚。也就是说，对离群点比较敏感，受其影响较大。

如果样本中存在离群点，MSE会给离群点更高的权重，这就会牺牲其他正常点数据的预测效果，最终降低整体模型性能。

![img](D:/oneDrive/typr/439761-20191211114501909-574033959.png)

可见，使用 MSE 损失函数，受离群点的影响较大，虽然样本中只有 5 个离群点，但是拟合的直线还是比较偏向于离群点。





### MSE和MAE的选择

- 从梯度的求解以及收敛上，MSE是优于MAE的。MSE处处可导，而且梯度值也是动态变化的，能够快速的收敛；而MAE在0点处不可导，且其梯度保持不变。对于很小的损失值其梯度也很大，在深度学习中，就需要使用变化的学习率，在损失值很小时降低学习率。
- 对离群（异常）值得处理上，MAE要明显好于MSE。

如果离群点（异常值）需要被检测出来，则可以选择MSE作为损失函数；如果离群点只是当做受损的数据处理，则可以选择MAE作为损失函数。

总之，MAE作为损失函数更稳定，并且对离群值不敏感，但是其导数不连续，求解效率低。另外，在深度学习中，收敛较慢。MSE导数求解速度高，但是其对离群值敏感，不过可以将离群值的导数设为0（导数值大于某个阈值）来避免这种情况。

在某些情况下，上述两种损失函数都不能满足需求。例如，若数据中90%的样本对应的目标值为150，剩下10%在0到30之间。那么使用MAE作为损失函数的模型可能会忽视10%的异常点，而对所有样本的预测值都为150。这是因为模型会按中位数来预测。而使用MSE的模型则会给出很多介于0到30的预测值，因为模型会向异常点偏移。

这种情况下，MSE和MAE都是不可取的，简单的办法是对目标变量进行变换，或者使用别的损失函数，例如：Huber,Log-Cosh以及分位数损失等。

# smooth L1 损失

在Faster R-CNN以及SSD中对边框的回归使用的损失函数都是Smooth L1L1 作为损失函数，





SmoothL1(x)={0.5x2∣x∣−0.5if∣x∣<1otherwiseSmoothL1(x)={0.5x2if∣x∣<1∣x∣−0.5otherwise



其中，x=f(xi)−yix=f(xi)−yi 为真实值和预测值的差值。

Smooth L1L1 能从两个方面限制梯度：

1. 当预测框与 ground truth 差别过大时，梯度值不至于过大；
2. 当预测框与 ground truth 差别很小时，梯度值足够小。

#### 对比L1L1 Loss 和 L2L2 Loss

其中xx为预测框与groud truth之间的差异：





L2(x)L1(x)smoothL1(x)=x2=x={0.5x2∣x∣−0.5if∣x∣<1otherwise(1)(2)(3)(1)L2(x)=x2(2)L1(x)=x(3)smoothL1(x)={0.5x2if∣x∣<1∣x∣−0.5otherwise



上面损失函数对xx的导数为：





∂L2(x)∂x∂L1(x)∂x∂smoothL1(x)∂x=2x={1−1if x≥0otherwise={x±1if∣x∣<1otherwise(4)(5)(6)(4)∂L2(x)∂x=2x(5)∂L1(x)∂x={1if x≥0−1otherwise(6)∂smoothL1(x)∂x={xif∣x∣<1±1otherwise



上面导数可以看出：

- 根据公式-4，当xx增大时，L2L2的损失也增大。 这就导致在训练初期，预测值与 groud truth 差异过于大时，损失函数对预测值的梯度十分大，训练不稳定。
- 根据公式-5,L1L1对xx的导数为常数，在训练的后期，预测值与ground truth差异很小时，L1L1的导数的绝对值仍然为1，而 learning rate 如果不变，损失函数将在稳定值附*波动，难以继续收敛以达到更高精度。
- 根据公式-6，Smotth L1Smotth L1在xx较小时，对xx的梯度也会变小。 而当xx较大时，对xx的梯度的上限为1，也不会太大以至于破坏网络参数。SmoothL1SmoothL1完美的避开了L1L1和L2L2作为损失函数的缺陷。

L1L1 Loss ,L2L2 Loss以及SmoothL1SmoothL1 放在一起的函数曲线对比

![untitled](D:/oneDrive/typr/untitled-16628866898982.png)

从上面可以看出，该函数实际上就是一个分段函数，在[-1,1]之间实际上就是L2损失，这样解决了L1的不光滑问题，在[-1,1]区间外，实际上就是L1损失，这样就解决了离群点梯度爆炸的问题

# 交叉熵损失

针对多分类问题（softmax输出，所有输出概率和为1）
$$
H=-\sum o_i^*\log(o_i)
$$
假设真实标签[0,0,...,1,...,0]

预测概率[0.1,0.3,...,0.4,...,0.1]
$$
Loss=-\log(0.4)
$$


针对二分类问题（sigmoid输出，每个输出节点之间互不相干）
$$
H=-\frac{1}{N}[o_i^*\log o_i+(1-o_i^*)\log (1-o_i)]
$$
$o_i^*$为真实标签值，$o_i$为预测值，log以e为底