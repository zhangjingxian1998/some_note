# BP神经网络

输入层只保存输入数据,不执行任何计算。因此不需要使用激活函数。

权值更新朝着梯度反方向，梯度下降，[机器学习算法 之 集成学习：GBDT_张之海的博客-CSDN博客](https://blog.csdn.net/ZZh1301051836/article/details/88825273)

分为正向传播和反向传播过程，除了数据输入层，每一层的输出都要经过激活函数。

> #### 前向传播过程

输入层，隐含层，激活层，输出层，依次顺序乘开。

输入层：$X$，是$M\times N$矩阵，$M$行数表示$M$个样本，$N$列数表示每个样本有$N$个特征。

输出层：$Y$

隐含层输入层：$H_L$，由上一层输入与权重矩阵决定。

隐含层输出层：$Z_L$，上一层经过激活函数得到下一层，维数与上一层保持一致。

权重矩阵：$W_L$，行数与上一层特征数相同，列数是下一层有多少特征。

偏置：$B_L$

激活函数：$F_L$





> #### 反向传播过程







# CNN

> #### 全连接层

实现分类（Classification），在很多分类问题需要通过softmax层进行输出

池化层的后面一般接着全连接层，全连接层将池化层的所有特征矩阵转化成一维的特征大向量，全连接层一般放在[卷积神经网络](https://so.csdn.net/so/search?q=卷积神经网络&spm=1001.2101.3001.7020)结构中的最后，用于对图片进行分类，到了全连接层，我们的神经网络就要准备输出结果了

> #### 卷积层

提取特征。

> #### 池化层

减小卷积核的尺寸，用来降维。

最大池化，平均池化。

池化层主要有以下几个作用：

1. 挑选不受位置干扰的图像信息。

2. 对特征进行降维，提高后续特征的感受野，也就是让池化后的一个像素对应前面图片中的一个区域。

3. 因为池化层是不进行反向传播的，而且池化层减少了特征图的变量个数，所以池化层可以减少计算量。

# 注意力机制（CNN）

> #### channel attention

对每个C，在channel维度上，学习到不同的权重，平面维度上权重相同。

> #### spatial attention

CNN每一层都会输出一个C x H x W的特征图，C就是通道，代表卷积核的数量，亦为特征的数量，H 和W就是原始图片经过压缩后的图，spatial attention就是对于所有的通道，在二维平面上，对H x W尺寸的图学习到一个权重，对每个像素都会学习到一个权重。你可以想象成一个像素是C维的一个向量，深度是C，在C个维度上，权重都是一样的，但是在平面上，权重不一样。

> #### 总结

spatial 和 channel attention可以理解为关注图片的不同区域和关注图片的不同特征。channel attention写的最好的一篇论文是SCA-CNN

attention机制听起来高达上，其实就是学出一个权重分布，再拿这个权重分布施加在原来的特征之上，就可以叫做attention。简单来说：
（1）这个加权可以是保留所有分量均做加权（即soft attention）；也可以是在分布中以某种采样策略选取部分分量（即hard attention）。

（3）这个加权可以作用在空间尺度上，给不同空间区域加权；也可以作用在channel尺度上，给不同通道特征加权；甚至特征图上每个元素加权。
（4）这个加权还可以作用在不同时刻历史特征上，如Machine Translation。

# RNN/LSTM

 已被证明是处理顺序数据时非常有效的模型

虽然是用来处理动态时序信息，但是经过专门设计，对静态输入建模也能显著提高网络集成上下文和减少预测误差的能力。（Room-Net）

# segNet

它具有时间和内存效率，并且在各种细分任务中显示出良好的性能。像大多数语义分割体系结构一样，它由两个子网络组成，一个编码器和一个解码器。编码器应用一系列卷积和池化操作，将输入图像映射到较低分辨率的特征图。然后，解码器对低分辨率特征图进行采样，以恢复到完整的图像分辨率，以进行像素精确分类。这是通过基于编码器中相应的池化指数的一系列非线性上采样操作来实现的。由于上采样的地图是稀疏的，因此将它们与可学习的过滤器进行卷积，以生成密集的特征地图。

# FPN

处理小物体检测和分割任务的最有效方法之一。

# 深度可分离卷积（DSC）

理论上可以减少计算参数，提高计算效率。由于内存带宽和数据IO的限制，深度可分离卷积在GPU上的速度并不快，其主要原因是内存访问量很高

[深度可分离卷积（Depthwise separable convolution） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/165632315)

[(3条消息) 深度可分离卷积_深度可分离卷积性能研究_weixin_39541681的博客-CSDN博客](https://blog.csdn.net/weixin_39541681/article/details/111382230)
