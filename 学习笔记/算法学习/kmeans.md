# kmeans

K-means算法是聚类分析中使用最广泛的算法之一。它把n个对象根据他们的属性分为k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。其聚类过程可以用下图表示

![img](D:/oneDrive/typr/20131024140835468.jpeg)

如图所示，数据样本用圆点表示，每个簇的中心点用叉叉表示。$(a)$刚开始时是原始数据，杂乱无章，没有$label$，看起来都一样，都是绿色的。$(b)$假设数据集可以分为两类，令$K=2$，即有两个簇类，随机在坐标上选两个点，作为两个类的中心点。$(c-f)$演示了聚类的两种迭代。先划分，把每个数据样本划分到最近的中心点那一簇；划分完后，更新每个簇的中心，即把该簇的所有数据点的坐标加起来去平均值。这样不断进行”划分—更新—划分—更新”，直到每个簇的中心不在移动或移动很少为止。

该算法过程比较简单，但有些东西我们还是需要关注一下，此处，我想说一下"求点中心的算法"

一般来说，求点群中心点的算法你可以很简的使用各个点的$X，Y$坐标的平均值。也可以用另三个求中心点的的公式：

1. Minkowski Distance

   λ 可以随意取值，可以是负数，也可以是正数，或是无穷大。
   $$
   d_{ij}=\sqrt[\lambda]{\sum_{k=1}^n|x_{ik}-x_{jk}|^\lambda}
   $$
   
2. Euclidean Distance

   也就是第一个公式 λ=2 的情况
   $$
   d_{ij}=\sqrt[2]{\sum_{k=1}^n|x_{ik}-x_{jk}|^2}
   $$
   
3. CityBlock Distance

   也就是第一个公式 λ=1 的情况
   $$
   d_{ij}=\sum_{k=1}^n|x_{ik}-x_{jk}|
   $$

这三个公式的求中心点有一些不一样的地方，我们看下图（对于第一个 λ 在 0-1之间）。



![img](D:/oneDrive/typr/Minkowski-Mean.jpg)

![img](D:/oneDrive/typr/Euclidean-distance.jpg)

![img](D:/oneDrive/typr/Manhattan-distance.jpg)

上面这几个图的大意是他们是怎么个逼近中心的，第一个图以星形的方式，第二个图以同心圆的方式，第三个图以菱形的方式。

# Kmeans算法的缺陷

- 聚类中心的个数$K$需要事先给定，但在实际中这个$K$值的选定是非常难以估计的，很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适
- $Kmeans$需要人为地确定初始聚类中心，不同的初始聚类中心可能导致完全不同的聚类结果。（可以使用$Kmeans++$算法来解决）

# K-Means++算法

 k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。

1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心
2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
4. 重复2和3直到k个聚类中心被选出来
5. 利用这k个初始的聚类中心来运行标准的k-means算法

从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下：

1. 先从我们的数据库随机挑个随机点当“种子点”
2. 对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。
3. 然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其<=0，此时的点就是下一个“种子点”。
4. 重复2和3直到k个聚类中心被选出来
5. 利用这k个初始的聚类中心来运行标准的k-means算法

可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。

# KNN

