{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "path = '/home/zhangjx/All_model/genration_scene/VL-T5/t5-base/pytorch_model.bin'\n",
    "weight = torch.load(path)['encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight']\n",
    "is_decoder = False\n",
    "relative_attention_num_buckets = 32\n",
    "relative_attention_max_distance = 128\n",
    "relative_attention_bias = nn.Embedding(32,12)\n",
    "relative_attention_bias.weight = Parameter(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "    relative_buckets = 0\n",
    "    if bidirectional:\n",
    "        num_buckets //= 2\n",
    "        relative_buckets += (relative_position > 0).to(torch.long) * num_buckets # 这个是做什么的\n",
    "        relative_position = torch.abs(relative_position)\n",
    "    else:\n",
    "        relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))\n",
    "    # print(relative_buckets)\n",
    "    # now relative_position is in the range [0, inf)\n",
    "\n",
    "    # half of the buckets are for exact increments in positions\n",
    "    max_exact = num_buckets // 2\n",
    "    is_small = relative_position < max_exact\n",
    "    # print(is_small)\n",
    "    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "    relative_position_if_large = max_exact + (\n",
    "        torch.log(relative_position.float() / max_exact)\n",
    "        / math.log(max_distance / max_exact)\n",
    "        * (num_buckets - max_exact)\n",
    "    ).to(torch.long)\n",
    "    # print(relative_position_if_large)\n",
    "    relative_position_if_large = torch.min(\n",
    "        relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)\n",
    "    ) # 截断\n",
    "    # tmp = relative_position_if_large\n",
    "    # tmp[tmp<0]=0\n",
    "    # print(tmp)\n",
    "    # print(is_small)\n",
    "    relative_buckets += torch.where(is_small, relative_position, relative_position_if_large) # 8个以内取原值，8个以外取大值\n",
    "    return relative_buckets\n",
    "\n",
    "def compute_bias(query_length, key_length, device=None):\n",
    "    \"\"\"Compute binned relative position bias\"\"\"\n",
    "    if device is None:\n",
    "        device = relative_attention_bias.weight.device\n",
    "    context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n",
    "    memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n",
    "    relative_position = memory_position - context_position  # shape (query_length, key_length)\n",
    "    relative_position_bucket = _relative_position_bucket(\n",
    "        relative_position,  # shape (query_length, key_length)\n",
    "        bidirectional=(not is_decoder),\n",
    "        num_buckets=relative_attention_num_buckets,\n",
    "        max_distance=relative_attention_max_distance,\n",
    "    )\n",
    "    print(relative_position_bucket)\n",
    "    values = relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)\n",
    "    values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_bias = compute_bias(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(position_bias[0,3].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    P = np.zeros((seq_len, d))\n",
    "    for t in range(seq_len):\n",
    "        for k in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*k/d)\n",
    "            P[t, 2*k] = np.sin(t/denominator)\n",
    "            P[t, 2*k+1] = np.cos(t/denominator)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = getPositionEncoding(seq_len=50, d=128, n=50)\n",
    "cax = plt.matshow(P,cmap=plt.cm.Blues)\n",
    "plt.gcf().colorbar(cax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
